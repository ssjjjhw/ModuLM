{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/deepseek/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from graph3d import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cz/MolTC-main/model\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# 添加新的路径\n",
    "sys.path.append('/work/home/ac15y5ara4/cz/MolTC-main/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/deepseek/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/envs/deepseek/lib/python3.10/site-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore\n",
      "/opt/conda/envs/deepseek/lib/python3.10/site-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore\n",
      "2025-03-17 10:07:21.542503: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742177241.569888   49365 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742177241.577969   49365 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-17 10:07:21.605718: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fused_multi_tensor is not installed corrected\n",
      "fused_rounding is not installed corrected\n",
      "fused_layer_norm is not installed corrected\n",
      "fused_softmax is not installed corrected\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    " Copyright (c) 2023, salesforce.com, inc.\n",
    " All rights reserved.\n",
    " SPDX-License-Identifier: BSD-3-Clause\n",
    " For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n",
    "\"\"\"\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast as autocast\n",
    "from torch.nn import functional as F\n",
    "from peft import get_peft_config, get_peft_model, get_peft_model_state_dict, LoraConfig, TaskType, PeftModel\n",
    "from ogb.utils import smiles2graph\n",
    "from torch_geometric.loader.dataloader import Collater\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "from lavis.models.blip2_models.blip2 import (\n",
    "    # Blip2Base,\n",
    "    disabled_train,\n",
    ")\n",
    "from model.blip2 import Blip2Base\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import OPTForCausalLM\n",
    "# from opendelta import LoraModel\n",
    "# from opendelta.delta_models.lora import LoraConfig\n",
    "# from opendelta.delta_configs\n",
    "\n",
    "opt_model_list = [\n",
    "    \"facebook/galactica-125m\",\n",
    "    \"facebook/galactica-1.3b\",\n",
    "    \"facebook/galactica-6.7b\",\n",
    "    \"facebook/galactica-30b\",\n",
    "]\n",
    "\n",
    "def mask_by_len(input, lens, fill_value=0):\n",
    "    '''\n",
    "    input: shape = [N, D]\n",
    "    lens: shape = [N]\n",
    "    '''\n",
    "    mask = torch.arange(input.shape[1], device=input.device).reshape(1, -1)\n",
    "    mask = mask < lens.reshape(-1, 1)\n",
    "    input[mask] = fill_value\n",
    "    return input\n",
    "\n",
    "\n",
    "def smiles2data(smiles):\n",
    "    graph = smiles2graph(smiles)\n",
    "    x = torch.from_numpy(graph['node_feat'])\n",
    "    edge_index = torch.from_numpy(graph['edge_index'], )\n",
    "    edge_attr = torch.from_numpy(graph['edge_feat'])\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    return data\n",
    "\n",
    "import re\n",
    "SPLIT_MARKER = f\"SPL{1}T-TH{1}S-Pl3A5E\"\n",
    "\n",
    "CUSTOM_SEQ_RE = re.compile(r\"(\\[START_(DNA|SMILES|I_SMILES|AMINO)])(.*?)(\\[END_\\2])\")\n",
    "\n",
    "\n",
    "def _insert_split_marker(m: re.Match):\n",
    "    \"\"\"\n",
    "    Applies split marker based on a regex match of special tokens such as\n",
    "    [START_DNA].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n : str\n",
    "        Input text to split\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    str - the text with the split token added\n",
    "    \"\"\"\n",
    "    start_token, _, sequence, end_token = m.groups()\n",
    "    sequence = re.sub(r\"(.)\", fr\"{SPLIT_MARKER}\\1\", sequence, flags=re.DOTALL)\n",
    "    return f\"{start_token}{sequence}{SPLIT_MARKER}{end_token}\"\n",
    "\n",
    "def escape_custom_split_sequence(text):\n",
    "    \"\"\"\n",
    "    Applies custom splitting to the text for GALILEO's tokenization\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        Input text to split\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    str - the text with the split token added\n",
    "    \"\"\"\n",
    "    return CUSTOM_SEQ_RE.sub(_insert_split_marker, text)\n",
    "\n",
    "def smiles_handler(text, mol_ph):\n",
    "    smiles_list = []\n",
    "    for match in CUSTOM_SEQ_RE.finditer(text):\n",
    "        smiles = match.group(3)\n",
    "        smiles_list.append(smiles)\n",
    "    \n",
    "    text = CUSTOM_SEQ_RE.sub(r'\\1\\3\\4%s' % (mol_ph), text)\n",
    "    text = escape_custom_split_sequence(text)\n",
    "    return text, smiles_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict\n",
    "\n",
    "args = EasyDict({\n",
    "    \"filename\": \"ft_ddi_value_stage2_new16\",\n",
    "    \"seed\": 42,\n",
    "    \"mode\": \"ft\",\n",
    "    \"strategy_name\": None,\n",
    "    \"iupac_prediction\": False,\n",
    "    \"ckpt_path\": None,\n",
    "    \"gin_hidden_dim\": 300,\n",
    "    \"gin_num_layers\": 5,\n",
    "    \"drop_ratio\": 0.0,\n",
    "    \"tune_gnn\": True,\n",
    "    \"bert_hidden_dim\": 768,\n",
    "    \"bert_name\": \"scibert\",\n",
    "    \"cross_attention_freq\": 2,\n",
    "    \"num_query_token\": 8,\n",
    "    \"opt_model\": \"facebook/galactica-1.3b\",\n",
    "    \"num_beams\": 5,\n",
    "    \"do_sample\": False,\n",
    "    \"max_len\": 30,\n",
    "    \"min_len\": 8,\n",
    "    \"llm_tune\": \"lora\",\n",
    "    \"peft_config\": None,\n",
    "    \"peft_dir\": \"\",\n",
    "    \"save_every_n_epochs\": 10,\n",
    "    \"load_in_8bit\": False,\n",
    "    \"lora_r\": 8,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"reaction_weight\": 1.0,\n",
    "    \"weight_decay\": 0.05,\n",
    "    \"init_lr\": 0.0001,\n",
    "    \"min_lr\": 1.0e-05,\n",
    "    \"warmup_lr\": 1.0e-06,\n",
    "    \"warmup_steps\": 1000,\n",
    "    \"lr_decay_rate\": 0.9,\n",
    "    \"scheduler\": \"linear_warmup_cosine_lr\",\n",
    "    \"stage1_path\": \"\",\n",
    "    \"stage2_path\": \"all_checkpoints/stage2/last.ckpt\",\n",
    "    \"init_checkpoint\": \"all_checkpoints/pretrain1/last.ckpt\",\n",
    "    \"caption_eval_epoch\": 50,\n",
    "    \"num_workers\": 2,\n",
    "    \"batch_size\": 20,\n",
    "    \"inference_batch_size\": 4,\n",
    "    \"use_smiles\": False,\n",
    "    \"root\": \"/home/cz/MolTC-main/data/ddi_data/Zhangddi_data/train/\",\n",
    "    \"text_max_len\": 128,\n",
    "    \"prompt\": \"[START_I_SMILES]{}[END_I_SMILES]. \",\n",
    "    \"accelerator\": \"gpu\",\n",
    "    \"devices\": \"0,1\",\n",
    "    \"precision\": \"bf16\",\n",
    "    \"max_epochs\": 1000,\n",
    "    \"accumulate_grad_batches\": 1,\n",
    "    \"check_val_every_n_epoch\": 1,\n",
    "    \"double\": True,\n",
    "    \"solve\": False,\n",
    "    \"valid_root\": \"/home/cz/MolTC-main/data/ddi_data/Zhangddi_data/valid/\",\n",
    "    \"DDI\": True,\n",
    "    \"fangguangtuan\": False,\n",
    "    \"zijiegou\":False\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:524: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
      "<>:693: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
      "<>:524: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
      "<>:693: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
      "/tmp/ipykernel_49365/3830296720.py:524: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
      "  device = XXXX-2(self.parameters()).device\n",
      "/tmp/ipykernel_49365/3830296720.py:693: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
      "  device = XXXX-2(self.parameters()).device\n"
     ]
    }
   ],
   "source": [
    "class Blip2OPT(Blip2Base):\n",
    "    \"\"\"\n",
    "    BLIP2 first-stage model with Q-former and ViT.\n",
    "    Supported model types:\n",
    "        - pretrained: pretrained model with vit-g\n",
    "        - pretrain_vitL: pretrained model with vit-large\n",
    "        - coco: fintuned model on coco\n",
    "    Usage:\n",
    "        >>> from lavis.models import load_model\n",
    "        >>> model = load_model(\"blip2\", \"pretrain\")\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        bert_name,\n",
    "        gin_num_layers,\n",
    "        gin_hidden_dim,\n",
    "        gin_drop_ratio,\n",
    "        tune_gnn=False,\n",
    "        num_query_token=32,\n",
    "        cross_attention_freq=2,\n",
    "        llm_tune='freeze',\n",
    "        peft_dir='',\n",
    "        opt_model=\"facebook/galactica-1.3b\",\n",
    "        prompt=\"\",\n",
    "        args=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "\n",
    "        self.graph_encoder, self.ln_graph = self.init_graph_encoder(gin_num_layers, gin_hidden_dim, gin_drop_ratio)\n",
    "        self.tune_gnn = tune_gnn\n",
    "        if not tune_gnn:\n",
    "            for name, param in self.graph_encoder.named_parameters():\n",
    "                param.requires_grad = False\n",
    "            self.graph_encoder = self.graph_encoder.eval()\n",
    "            self.graph_encoder.train = disabled_train\n",
    "            logging.info(\"freeze graph encoder\")\n",
    "        \n",
    "        self.num_query_token = num_query_token\n",
    "        self.Qformer, self.query_tokens = self.init_Qformer(bert_name, num_query_token, self.graph_encoder.num_features, cross_attention_freq)\n",
    "        ### remove the unused parameters\n",
    "        self.Qformer.cls = None\n",
    "        self.Qformer.bert.embeddings.word_embeddings = None\n",
    "        self.Qformer.bert.embeddings.position_embeddings = None\n",
    "        for layer in self.Qformer.bert.encoder.layer:\n",
    "            layer.output = None\n",
    "            layer.intermediate = None\n",
    "        #####################################\n",
    "        ## initialize opt model\n",
    "\n",
    "        # opt_model = \"./galactica-1.3b\"\n",
    "        opt_model = \"/work/home/ac15y5ara4/cz/MolTC-main/deepseek/falcon/7b\"\n",
    "        self.opt_tokenizer = AutoTokenizer.from_pretrained(opt_model, use_fast=False, padding_side='right')\n",
    "        self.opt_tokenizer.add_special_tokens({'pad_token': '<pad>', 'sep_token': '</s>'})\n",
    "        self.opt_tokenizer.add_tokens('<mol>') # molecule placeholder\n",
    "        self.mol_token = '<mol>'\n",
    "        self.opt_tokenizer.mol_token_id = self.opt_tokenizer(\"<mol>\", add_special_tokens=False).input_ids[0]\n",
    "\n",
    "        self.collater = Collater([], [])\n",
    "        \n",
    "        if opt_model == 'facebook/galactica-125m':\n",
    "            self.opt_model = OPTForCausalLM.from_pretrained(opt_model)\n",
    "        else:\n",
    "            # self.opt_model = OPTForCausalLM.from_pretrained(opt_model, torch_dtype=torch.float16)\n",
    "            self.opt_model = OPTForCausalLM.from_pretrained(opt_model, torch_dtype=torch.bfloat16)\n",
    "        self.opt_model.resize_token_embeddings(len(self.opt_tokenizer)) ## this will cause bug when full fine-tuning the opt model\n",
    "\n",
    "        self.llm_tune = llm_tune\n",
    "        if llm_tune == 'lora':\n",
    "            if peft_dir:\n",
    "                self.opt_model = PeftModel.from_pretrained(self.opt_model, peft_dir, is_trainable=True)\n",
    "                #for name, param in self.opt_model.named_parameters():\n",
    "                #    param.requires_grad = False\n",
    "            else:\n",
    "                if self.args.peft_config:\n",
    "                    peft_config = LoraConfig(**LoraConfig.from_json_file(self.args.peft_config))\n",
    "                else:\n",
    "                    peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, inference_mode=False, r=args.lora_r, lora_alpha=args.lora_alpha, lora_dropout=args.lora_dropout)\n",
    "                self.peft_config = peft_config\n",
    "                self.opt_model = get_peft_model(self.opt_model, peft_config)\n",
    "            self.opt_model.print_trainable_parameters()\n",
    "        elif llm_tune == 'freeze':\n",
    "            for name, param in self.opt_model.named_parameters():\n",
    "                param.requires_grad = False\n",
    "        elif llm_tune == 'full':\n",
    "            pass\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        ## fixme: this is different from the original BLIP2\n",
    "        self.eos_token_id = self.opt_tokenizer(\n",
    "            \"\\n\", add_special_tokens=False\n",
    "        ).input_ids[0]\n",
    "\n",
    "        self.opt_proj = nn.Linear(\n",
    "            self.Qformer.config.hidden_size, self.opt_model.config.hidden_size\n",
    "        )\n",
    "        \n",
    "        ## fixme: no prompt yet\n",
    "        self.prompt = prompt\n",
    "        # prompt_tokens = self.opt_tokenizer(self.prompt, return_tensors=\"pt\")\n",
    "        # self.prompt_length = prompt_tokens.attention_mask.sum(1)\n",
    "\n",
    "    def forward_old(self, batch):\n",
    "        graphs, text_tokens, prompt_lens = batch\n",
    "        graph_embeds, graph_masks = self.graph_encoder(graphs)\n",
    "        if not self.tune_gnn:\n",
    "            graph_embeds = graph_embeds.detach()\n",
    "        graph_embeds = self.ln_graph(graph_embeds, graph_masks)\n",
    "        device = graph_embeds.device\n",
    "        query_tokens = self.query_tokens.expand(graph_embeds.shape[0], -1, -1)\n",
    "        query_output = self.Qformer.bert(\n",
    "            query_embeds=query_tokens,\n",
    "            encoder_hidden_states=graph_embeds,\n",
    "            encoder_attention_mask=graph_masks, # fixme: check whether this mask is correct\n",
    "            return_dict=True,\n",
    "        )\n",
    "        inputs_opt = self.opt_proj(query_output.last_hidden_state)\n",
    "        atts_opt = torch.ones(inputs_opt.size()[:-1], dtype=torch.long).to(device)\n",
    "        targets = text_tokens.input_ids.masked_fill(\n",
    "            text_tokens.input_ids == self.opt_tokenizer.pad_token_id, -100\n",
    "        )\n",
    "        if self.prompt:\n",
    "            targets = mask_by_len(targets, prompt_lens, -100) # do not apply loss to the prompt\n",
    "            # targets[:, : self.prompt_length] = -100  # do not apply loss to the prompt\n",
    "        \n",
    "        empty_targets = (\n",
    "            torch.ones(atts_opt.size(), dtype=torch.long).to(device).fill_(-100)\n",
    "        )\n",
    "        targets = torch.cat([empty_targets, targets], dim=1)\n",
    "        \n",
    "        inputs_embeds = self.opt_model.get_input_embeddings()(text_tokens.input_ids)\n",
    "        inputs_embeds = torch.cat([inputs_opt, inputs_embeds], dim=1)\n",
    "        attention_mask = torch.cat([atts_opt, text_tokens.attention_mask], dim=1)\n",
    "        \n",
    "        outputs = self.opt_model(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True,\n",
    "            labels=targets,\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        return {\"loss\": loss}\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        if self.args.double==False and self.args.solve == False and self.args.fangguangtuan == False and self.args.DDI == False:\n",
    "            graphs, prompt_tokens, text_tokens = batch\n",
    "            #print(graphs)pyg图数据格式\n",
    "            #print(prompt_tokens)下面两个都是64 54 38这种\n",
    "            #print(text_tokens)\n",
    "            graph_embeds, graph_masks = self.graph_encoder(graphs)\n",
    "            if not self.tune_gnn:\n",
    "                graph_embeds = graph_embeds.detach()\n",
    "            graph_embeds = self.ln_graph(graph_embeds, graph_masks)\n",
    "            #print(graph_embeds)应该是读出完之后的向量了\n",
    "            device = graph_embeds.device\n",
    "            query_tokens = self.query_tokens.expand(graph_embeds.shape[0], -1, -1)\n",
    "            #print(query_tokens)也是向量罢了\n",
    "            query_output = self.Qformer.bert(\n",
    "                query_embeds=query_tokens,\n",
    "                encoder_hidden_states=graph_embeds,\n",
    "                encoder_attention_mask=graph_masks, # fixme: check whether this mask is correct\n",
    "                return_dict=True,\n",
    "            )\n",
    "            mol_tokens = self.opt_proj(query_output.last_hidden_state)\n",
    "            #print(mol_tokens)\n",
    "            #这里应该是去取出了分子的向量,并不是token\n",
    "            empty_targets = torch.ones(prompt_tokens.attention_mask.shape, dtype=torch.long).to(device).fill_(-100)\n",
    "            targets = text_tokens.input_ids.masked_fill(\n",
    "                text_tokens.input_ids == self.opt_tokenizer.pad_token_id, -100\n",
    "            )\n",
    "            targets = torch.cat([empty_targets, targets], dim=1)\n",
    "            prompt_embeds = self.opt_model.get_input_embeddings()(prompt_tokens.input_ids)\n",
    "            prompt_embeds[prompt_tokens.is_mol_token] = mol_tokens.flatten(0, 1)\n",
    "            inputs_embeds = self.opt_model.get_input_embeddings()(text_tokens.input_ids)\n",
    "            inputs_embeds = torch.cat((prompt_embeds, inputs_embeds), dim=1)\n",
    "            attention_mask = torch.cat([prompt_tokens.attention_mask, text_tokens.attention_mask], dim=1)\n",
    "            #print(targets)\n",
    "            outputs = self.opt_model(\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                attention_mask=attention_mask,\n",
    "                return_dict=True,\n",
    "                labels=targets,\n",
    "            ) # type: ignore\n",
    "            loss = outputs.loss\n",
    "            return {\"loss\": loss}\n",
    "        else:\n",
    "            graphs1, graphs2,prompt_tokens, text_tokens= batch\n",
    "            #print(graphs)pyg图数据格式\n",
    "            #print(prompt_tokens)下面两个都是64 54 38这种\n",
    "            #print(text_tokens)\n",
    "            graph_embeds1, graph_masks1 = self.graph_encoder(graphs1)\n",
    "            if not self.tune_gnn:\n",
    "                graph_embeds1 = graph_embeds1.detach()\n",
    "            graph_embeds1 = self.ln_graph(graph_embeds1, graph_masks1)\n",
    "            #print(graph_embeds)应该是读出完之后的向量了\n",
    "            device = graph_embeds1.device\n",
    "            query_tokens1 = self.query_tokens.expand(graph_embeds1.shape[0], -1, -1)\n",
    "            #print(query_tokens1.size())#也是向量罢了\n",
    "            query_output1 = self.Qformer.bert(\n",
    "                query_embeds=query_tokens1,\n",
    "                encoder_hidden_states=graph_embeds1,\n",
    "                encoder_attention_mask=graph_masks1, # fixme: check whether this mask is correct\n",
    "                return_dict=True,\n",
    "            )\n",
    "            #print(query_output1.size())\n",
    "            mol_tokens1 = self.opt_proj(query_output1.last_hidden_state)\n",
    "            #print(mol_tokens1.size())\n",
    "            #这里应该是去取出了分子的向量,并不是token，这里是将768层 变换称了2048层\n",
    "            \n",
    "            \n",
    "            \n",
    "            graph_embeds2, graph_masks2 = self.graph_encoder(graphs2)\n",
    "            if not self.tune_gnn:\n",
    "                graph_embeds2 = graph_embeds1.detach()\n",
    "            graph_embeds2 = self.ln_graph(graph_embeds2, graph_masks2)\n",
    "            #print(graph_embeds)应该是读出完之后的向量了\n",
    "            device = graph_embeds2.device\n",
    "            query_tokens2 = self.query_tokens.expand(graph_embeds2.shape[0], -1, -1)\n",
    "            #print(query_tokens)也是向量罢了\n",
    "            query_output2 = self.Qformer.bert(\n",
    "                query_embeds=query_tokens2,\n",
    "                encoder_hidden_states=graph_embeds2,\n",
    "                encoder_attention_mask=graph_masks2, # fixme: check whether this mask is correct\n",
    "                return_dict=True,\n",
    "            )\n",
    "            mol_tokens2 = self.opt_proj(query_output2.last_hidden_state)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            mol_tokens = torch.cat([mol_tokens1,mol_tokens2], dim=1)\n",
    "            empty_targets = torch.ones(prompt_tokens.attention_mask.shape, dtype=torch.long).to(device).fill_(-100)\n",
    "            targets = text_tokens.input_ids.masked_fill(\n",
    "                text_tokens.input_ids == self.opt_tokenizer.pad_token_id, -100\n",
    "            )\n",
    "            targets = torch.cat([empty_targets, targets], dim=1)\n",
    "            prompt_embeds = self.opt_model.get_input_embeddings()(prompt_tokens.input_ids)\n",
    "            prompt_embeds[prompt_tokens.is_mol_token] = mol_tokens.flatten(0, 1)\n",
    "            inputs_embeds = self.opt_model.get_input_embeddings()(text_tokens.input_ids)\n",
    "            inputs_embeds = torch.cat((prompt_embeds, inputs_embeds), dim=1)\n",
    "            attention_mask = torch.cat([prompt_tokens.attention_mask, text_tokens.attention_mask], dim=1)\n",
    "            \n",
    "            \n",
    "            outputs = self.opt_model(\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                attention_mask=attention_mask,\n",
    "                return_dict=True,\n",
    "                labels=targets,\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            return {\"loss\": loss}\n",
    "\n",
    "    def forward_reaction(self, batch):\n",
    "        reaction_tokens, notes_tokens, graphs = batch\n",
    "        graph_embeds, graph_masks = self.graph_encoder(graphs)\n",
    "        if not self.tune_gnn:\n",
    "            graph_embeds = graph_embeds.detach()\n",
    "        graph_embeds = self.ln_graph(graph_embeds, graph_masks)\n",
    "        device = graph_embeds.device\n",
    "        query_tokens = self.query_tokens.expand(graph_embeds.shape[0], -1, -1)\n",
    "        query_output = self.Qformer.bert(\n",
    "            query_embeds=query_tokens,\n",
    "            encoder_hidden_states=graph_embeds,\n",
    "            encoder_attention_mask=graph_masks, # fixme: check whether this mask is correct\n",
    "            return_dict=True,\n",
    "        )\n",
    "        mol_tokens = self.opt_proj(query_output.last_hidden_state) # shape = [mol_num, num_query_token, D]\n",
    "\n",
    "        if False:\n",
    "            if self.llm_tune:\n",
    "                react_embeds = self.opt_model.model.get_decoder().embed_tokens(reaction_tokens.input_ids) # shape = [B, max_len, D]\n",
    "                notes_embeds = self.opt_model.model.get_decoder().embed_tokens(notes_tokens.input_ids)\n",
    "            else:\n",
    "                react_embeds = self.opt_model.model.decoder.embed_tokens(reaction_tokens.input_ids) # shape = [B, max_len, D]\n",
    "                notes_embeds = self.opt_model.model.decoder.embed_tokens(notes_tokens.input_ids) # shape = [B, max_len, D]\n",
    "        else:\n",
    "            react_embeds = self.opt_model.get_input_embeddings()(reaction_tokens.input_ids)\n",
    "            notes_embeds = self.opt_model.get_input_embeddings()(notes_tokens.input_ids)\n",
    "\n",
    "        react_embeds[reaction_tokens.is_ph_token] = mol_tokens.flatten(0, 1)\n",
    "        inputs_embeds = torch.cat((react_embeds, notes_embeds), dim=1)\n",
    "\n",
    "        targets = notes_tokens.input_ids.masked_fill(\n",
    "            notes_tokens.input_ids == self.opt_tokenizer.pad_token_id, -100\n",
    "        )\n",
    "        empty_targets = (\n",
    "            torch.ones(reaction_tokens.attention_mask.shape, dtype=torch.long).to(device).fill_(-100)\n",
    "        )\n",
    "        targets = torch.cat([empty_targets, targets], dim=1)\n",
    "        attention_mask = torch.cat([reaction_tokens.attention_mask, notes_tokens.attention_mask], dim=1)\n",
    "\n",
    "        outputs = self.opt_model(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True,\n",
    "            labels=targets,\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_old(\n",
    "        self,\n",
    "        samples,\n",
    "        do_sample=False,\n",
    "        num_beams=5,\n",
    "        max_length=128,\n",
    "        min_length=1,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.0,\n",
    "        length_penalty=1.0,\n",
    "        num_captions=1,\n",
    "        temperature=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            samples (dict): A dictionary containing the following keys:\n",
    "                - image (torch.Tensor): A tensor of shape (batch_size, 3, H, W)\n",
    "            num_beams (int): Number of beams for beam search. 1 means no beam search.\n",
    "            max_length (int): The maximum length of the sequence to be generated.\n",
    "            min_length (int): The minimum length of the sequence to be generated.\n",
    "            top_p (float): The cumulative probability for nucleus sampling.\n",
    "            repetition_penalty (float): The parameter for repetition penalty. 1.0 means no penalty.\n",
    "            num_captions (int): Number of captions to be generated for each image.\n",
    "        Returns:\n",
    "            captions (list): A list of strings of length batch_size * num_captions.\n",
    "        \"\"\"\n",
    "        graphs = samples['graphs']\n",
    "        prompt_tokens = samples['prompt_tokens']\n",
    "        # prompt_lens = samples['prompt_lens']\n",
    "        with self.maybe_autocast():\n",
    "            graph_embeds, graph_masks = self.graph_encoder(graphs)\n",
    "            graph_embeds = self.ln_graph(graph_embeds)\n",
    "\n",
    "            query_tokens = self.query_tokens.expand(graph_embeds.shape[0], -1, -1)\n",
    "            query_output = self.Qformer.bert(\n",
    "                query_embeds=query_tokens,\n",
    "                encoder_hidden_states=graph_embeds,\n",
    "                encoder_attention_mask=graph_masks,\n",
    "                return_dict=True,\n",
    "            )\n",
    "\n",
    "            device = graph_embeds.device\n",
    "            inputs_opt = self.opt_proj(query_output.last_hidden_state)\n",
    "            atts_opt = torch.ones(inputs_opt.size()[:-1], dtype=torch.long, device=device)\n",
    "\n",
    "            attention_mask = torch.cat([atts_opt, prompt_tokens.attention_mask], dim=1)\n",
    "            \n",
    "            inputs_embeds = self.opt_model.get_input_embeddings()(prompt_tokens.input_ids)\n",
    "            inputs_embeds = torch.cat([inputs_opt, inputs_embeds], dim=1)\n",
    "\n",
    "            outputs = self.opt_model.generate(\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                attention_mask=attention_mask,\n",
    "                do_sample=do_sample,\n",
    "                top_p=top_p,\n",
    "                temperature=temperature,\n",
    "                num_beams=num_beams,\n",
    "                max_length=max_length,\n",
    "                min_length=min_length,\n",
    "                # pad_token_id=self.pad_token_id,\n",
    "                eos_token_id=self.eos_token_id,\n",
    "                repetition_penalty=repetition_penalty,\n",
    "                length_penalty=length_penalty,\n",
    "                num_return_sequences=num_captions,\n",
    "                return_dict_in_generate=True\n",
    "                # use_cache=False,\n",
    "            )\n",
    "            return outputs\n",
    "            output_text = self.opt_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            \n",
    "            output_text = [text.strip() for text in output_text]\n",
    "            return output_text\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        samples,\n",
    "        # max_new_tokens=30,\n",
    "        do_sample=False,\n",
    "        num_beams=5,\n",
    "        max_length=128,\n",
    "        min_length=1,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.0,\n",
    "        length_penalty=1.0,\n",
    "        num_captions=1,\n",
    "        temperature=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            samples (dict): A dictionary containing the following keys:\n",
    "                - image (torch.Tensor): A tensor of shape (batch_size, 3, H, W)\n",
    "            num_beams (int): Number of beams for beam search. 1 means no beam search.\n",
    "            max_length (int): The maximum length of the sequence to be generated.\n",
    "            min_length (int): The minimum length of the sequence to be generated.\n",
    "            top_p (float): The cumulative probability for nucleus sampling.\n",
    "            repetition_penalty (float): The parameter for repetition penalty. 1.0 means no penalty.\n",
    "            num_captions (int): Number of captions to be generated for each image.\n",
    "        Returns:\n",
    "            captions (list): A list of strings of length batch_size * num_captions.\n",
    "        \"\"\"\n",
    "        print(\"开始生成\")\n",
    "        if self.args.double == True or self.args.solve == True or self.args.fangguangtuan == True or self.args.DDI == True:\n",
    "            graphs1 = samples['graphs1']\n",
    "            prompt_tokens = samples['prompt_tokens']\n",
    "            # prompt_lens = samples['prompt_lens']\n",
    "            # with self.maybe_autocast():\n",
    "            graph_embeds1, graph_masks1 = self.graph_encoder(graphs1)\n",
    "            graph_embeds1 = self.ln_graph(graph_embeds1)\n",
    "\n",
    "            query_tokens1 = self.query_tokens.expand(graph_embeds1.shape[0], -1, -1)\n",
    "            query_output1 = self.Qformer.bert(\n",
    "                query_embeds=query_tokens1,\n",
    "                encoder_hidden_states=graph_embeds1,\n",
    "                encoder_attention_mask=graph_masks1,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            mol_tokens1 = self.opt_proj(query_output1.last_hidden_state)\n",
    "            \n",
    "            graphs2 = samples['graphs2']\n",
    "            # prompt_lens = samples['prompt_lens']\n",
    "            # with self.maybe_autocast():\n",
    "            graph_embeds2, graph_masks2 = self.graph_encoder(graphs2)\n",
    "            graph_embeds2 = self.ln_graph(graph_embeds2)\n",
    "\n",
    "            query_tokens2 = self.query_tokens.expand(graph_embeds2.shape[0], -1, -1)\n",
    "            query_output2 = self.Qformer.bert(\n",
    "                query_embeds=query_tokens2,\n",
    "                encoder_hidden_states=graph_embeds2,\n",
    "                encoder_attention_mask=graph_masks2,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            mol_tokens2 = self.opt_proj(query_output2.last_hidden_state)\n",
    "            mol_tokens=torch.cat([mol_tokens1,mol_tokens2],dim=1)\n",
    "            prompt_embeds = self.opt_model.get_input_embeddings()(prompt_tokens.input_ids)\n",
    "\n",
    "            mol_tokens = mol_tokens.to(prompt_embeds.dtype)\n",
    "\n",
    "            prompt_embeds[prompt_tokens.is_mol_token] = mol_tokens.flatten(0, 1)\n",
    "\n",
    "            outputs = self.opt_model.generate(\n",
    "                inputs_embeds=prompt_embeds,\n",
    "                attention_mask=prompt_tokens.attention_mask,\n",
    "                do_sample=do_sample,\n",
    "                top_p=top_p,\n",
    "                temperature=temperature,\n",
    "                num_beams=num_beams,\n",
    "                max_length=max_length,\n",
    "                min_length=min_length,\n",
    "                # pad_token_id=self.pad_token_id,\n",
    "                eos_token_id=self.eos_token_id,\n",
    "                repetition_penalty=repetition_penalty,\n",
    "                length_penalty=length_penalty,\n",
    "                num_return_sequences=num_captions,\n",
    "                output_hidden_states=True,\n",
    "                return_dict_in_generate=True\n",
    "                # use_cache=False,\n",
    "            )\n",
    "            return outputs\n",
    "            output_text = self.opt_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            \n",
    "            output_text = [text.strip() for text in output_text]\n",
    "            return output_text\n",
    "        else :\n",
    "            graphs = samples['graphs']\n",
    "            prompt_tokens = samples['prompt_tokens']\n",
    "            # prompt_lens = samples['prompt_lens']\n",
    "            # with self.maybe_autocast():\n",
    "            graph_embeds, graph_masks = self.graph_encoder(graphs)\n",
    "            graph_embeds = self.ln_graph(graph_embeds)\n",
    "\n",
    "            query_tokens = self.query_tokens.expand(graph_embeds.shape[0], -1, -1)\n",
    "            query_output = self.Qformer.bert(\n",
    "                query_embeds=query_tokens,\n",
    "                encoder_hidden_states=graph_embeds,\n",
    "                encoder_attention_mask=graph_masks,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            mol_tokens = self.opt_proj(query_output.last_hidden_state)\n",
    "            \n",
    "            prompt_embeds = self.opt_model.get_input_embeddings()(prompt_tokens.input_ids)\n",
    "            prompt_embeds[prompt_tokens.is_mol_token] = mol_tokens.flatten(0, 1)\n",
    "\n",
    "            outputs = self.opt_model.generate(\n",
    "                inputs_embeds=prompt_embeds,\n",
    "                attention_mask=prompt_tokens.attention_mask,\n",
    "                do_sample=do_sample,\n",
    "                top_p=top_p,\n",
    "                temperature=temperature,\n",
    "                num_beams=num_beams,\n",
    "                max_length=max_length,\n",
    "                min_length=min_length,\n",
    "                # pad_token_id=self.pad_token_id,\n",
    "                eos_token_id=self.eos_token_id,\n",
    "                repetition_penalty=repetition_penalty,\n",
    "                length_penalty=length_penalty,\n",
    "                num_return_sequences=num_captions,\n",
    "                # use_cache=False,\n",
    "            )\n",
    "            output_text = self.opt_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            \n",
    "            output_text = [text.strip() for text in output_text]\n",
    "            return output_text\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def blip_qa(\n",
    "        self, \n",
    "        samples,\n",
    "        do_sample=False,\n",
    "        num_beams=5,\n",
    "        max_length=128,\n",
    "        min_length=1,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.0,\n",
    "        length_penalty=1.0,\n",
    "        num_captions=1,\n",
    "        temperature=1,\n",
    "        output_scores=False,\n",
    "        ):\n",
    "\n",
    "        device = XXXX-2(self.parameters()).device\n",
    "        \n",
    "        ## data processing\n",
    "        prompts = samples['prompts'] # assume list of strings\n",
    "        prepared_prompts = []\n",
    "        mol_list = []\n",
    "        for p in prompts:\n",
    "            text, smiles = smiles_handler(p, self.mol_token * self.num_query_token)\n",
    "            prepared_prompts.append(text)\n",
    "            mol_list.extend([smiles2data(s) for s in smiles])\n",
    "        \n",
    "        prompt_tokens = self.opt_tokenizer(prepared_prompts,\n",
    "                                           truncation=False,\n",
    "                                           padding='longest',\n",
    "                                           add_special_tokens=True,\n",
    "                                        #    max_length=self.args.max_len[],\n",
    "                                           return_tensors='pt',\n",
    "                                           return_attention_mask=True).to(device)\n",
    "        \n",
    "        ## forward function\n",
    "        prompt_embeds = self.opt_model.get_input_embeddings()(prompt_tokens.input_ids)\n",
    "        \n",
    "        if len(mol_list) > 0:\n",
    "            graphs = self.collater(mol_list).to(device)\n",
    "            is_mol_token = (prompt_tokens.input_ids == self.mol_token) # shape = [B, max_len]\n",
    "            ## graph forward\n",
    "            graph_embeds, graph_masks = self.graph_encoder(graphs)\n",
    "            graph_embeds = self.ln_graph(graph_embeds, graph_masks)\n",
    "            query_tokens = self.query_tokens.expand(graph_embeds.shape[0], -1, -1)\n",
    "            query_output = self.Qformer.bert(\n",
    "                query_embeds=query_tokens,\n",
    "                encoder_hidden_states=graph_embeds,\n",
    "                encoder_attention_mask=graph_masks, # fixme: check whether this mask is correct\n",
    "                return_dict=True,\n",
    "            )\n",
    "            mol_tokens = self.opt_proj(query_output.last_hidden_state) # shape = [mol_num, num_query_token, D]\n",
    "            ## replace mol tokens\n",
    "            prompt_embeds[is_mol_token] = mol_tokens.flatten(0, 1)\n",
    "        \n",
    "        if output_scores:\n",
    "            outputs = self.opt_model.generate(\n",
    "                    inputs_embeds=prompt_embeds,\n",
    "                    attention_mask=prompt_tokens.attention_mask,\n",
    "                    do_sample=do_sample,\n",
    "                    top_p=top_p,\n",
    "                    temperature=temperature,\n",
    "                    num_beams=num_beams,\n",
    "                    max_length=max_length,\n",
    "                    min_length=min_length,\n",
    "                    # pad_token_id=self.pad_token_id,\n",
    "                    eos_token_id=self.eos_token_id,\n",
    "                    repetition_penalty=repetition_penalty,\n",
    "                    length_penalty=length_penalty,\n",
    "                    num_return_sequences=num_captions,\n",
    "                    output_scores=True,\n",
    "                    return_dict_in_generate=True\n",
    "                    # use_cache=False,\n",
    "            )\n",
    "            return outputs\n",
    "        else:\n",
    "            outputs = self.opt_model.generate(\n",
    "                    inputs_embeds=prompt_embeds,\n",
    "                    attention_mask=prompt_tokens.attention_mask,\n",
    "                    do_sample=do_sample,\n",
    "                    top_p=top_p,\n",
    "                    temperature=temperature,\n",
    "                    num_beams=num_beams,\n",
    "                    max_length=max_length,\n",
    "                    min_length=min_length,\n",
    "                    # pad_token_id=self.pad_token_id,\n",
    "                    eos_token_id=self.eos_token_id,\n",
    "                    repetition_penalty=repetition_penalty,\n",
    "                    length_penalty=length_penalty,\n",
    "                    num_return_sequences=num_captions,\n",
    "                    # use_cache=False,\n",
    "                )\n",
    "            output_text = self.opt_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            output_text = [text.strip() for text in output_text]\n",
    "            return output_text\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def opt_qa(\n",
    "        self, \n",
    "        samples,\n",
    "        do_sample=False,\n",
    "        num_beams=5,\n",
    "        max_length=128,\n",
    "        min_length=1,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.0,\n",
    "        length_penalty=1.0,\n",
    "        num_captions=1,\n",
    "        temperature=1,\n",
    "        output_scores=False,\n",
    "        ):\n",
    "\n",
    "        device = (self.parameters()).device\n",
    "        ## data processing\n",
    "        prompts = samples['prompts'] # assume list of strings\n",
    "        prompts = [escape_custom_split_sequence(p) for p in prompts]\n",
    "        \n",
    "        prompt_tokens = self.opt_tokenizer(prompts,\n",
    "                                           truncation=False,\n",
    "                                           padding='longest',\n",
    "                                           add_special_tokens=True,\n",
    "                                        #    max_length=self.args.max_len[],\n",
    "                                           return_tensors='pt',\n",
    "                                           return_attention_mask=True).to(device)\n",
    "        \n",
    "        prompt_embeds = self.opt_model.get_input_embeddings()(prompt_tokens.input_ids)\n",
    "\n",
    "        if output_scores:\n",
    "            ## forward function\n",
    "            outputs = self.opt_model.generate(\n",
    "                    inputs_embeds=prompt_embeds,\n",
    "                    attention_mask=prompt_tokens.attention_mask,\n",
    "                    do_sample=do_sample,\n",
    "                    top_p=top_p,\n",
    "                    temperature=temperature,\n",
    "                    num_beams=num_beams,\n",
    "                    max_length=max_length,\n",
    "                    min_length=min_length,\n",
    "                    # pad_token_id=self.pad_token_id,\n",
    "                    eos_token_id=self.eos_token_id,\n",
    "                    repetition_penalty=repetition_penalty,\n",
    "                    length_penalty=length_penalty,\n",
    "                    num_return_sequences=num_captions,\n",
    "                    # use_cache=False,\n",
    "                    output_scores=True,\n",
    "                    return_dict_in_generate=True\n",
    "                )\n",
    "            return outputs\n",
    "        else:\n",
    "            ## forward function\n",
    "            outputs = self.opt_model.generate(\n",
    "                    inputs_embeds=prompt_embeds,\n",
    "                    attention_mask=prompt_tokens.attention_mask,\n",
    "                    do_sample=do_sample,\n",
    "                    top_p=top_p,\n",
    "                    temperature=temperature,\n",
    "                    num_beams=num_beams,\n",
    "                    max_length=max_length,\n",
    "                    min_length=min_length,\n",
    "                    # pad_token_id=self.pad_token_id,\n",
    "                    eos_token_id=self.eos_token_id,\n",
    "                    repetition_penalty=repetition_penalty,\n",
    "                    length_penalty=length_penalty,\n",
    "                    num_return_sequences=num_captions,\n",
    "                    # use_cache=False,\n",
    "                )\n",
    "            output_text = self.opt_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            output_text = [text.strip() for text in output_text]\n",
    "            return output_text\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def probe_qformer(\n",
    "        self, \n",
    "        batch,\n",
    "        do_sample=False,\n",
    "        num_beams=5,\n",
    "        max_length=128,\n",
    "        min_length=1,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.0,\n",
    "        length_penalty=1.0,\n",
    "        num_captions=1,\n",
    "        temperature=1,\n",
    "        ):\n",
    "        with self.maybe_autocast():\n",
    "            device = XXXX-2(self.parameters()).device\n",
    "            \n",
    "            graphs, smiles_prompt_tokens, texts = batch\n",
    "            graphs = graphs.to(device)\n",
    "            ## graph forward\n",
    "            graph_embeds, graph_masks = self.graph_encoder(graphs)\n",
    "            graph_embeds = self.ln_graph(graph_embeds, graph_masks)\n",
    "            query_tokens = self.query_tokens.expand(graph_embeds.shape[0], -1, -1)\n",
    "            query_output = self.Qformer.bert(\n",
    "                query_embeds=query_tokens,\n",
    "                encoder_hidden_states=graph_embeds,\n",
    "                encoder_attention_mask=graph_masks, # fixme: check whether this mask is correct\n",
    "                return_dict=True,\n",
    "            )\n",
    "            mol_tokens = self.opt_proj(query_output.last_hidden_state) # shape = [mol_num, num_query_token, D]\n",
    "            B, num_q, D = mol_tokens.shape\n",
    "            \n",
    "            ## \n",
    "            embed_func = self.opt_model.get_input_embeddings()\n",
    "            embed_weight = embed_func.weight # shape = [vocab_size, D]\n",
    "            \n",
    "            dis_metric = 'cos'\n",
    "            topk = 10\n",
    "            if dis_metric == 'cos':\n",
    "                mol_tokens = F.normalize(mol_tokens, dim=-1, p=2)\n",
    "                embed_weight = F.normalize(embed_weight, dim=-1, p=2)\n",
    "                sim = mol_tokens.flatten(0, 1) @ embed_weight.T # shape = [mol_num * num_query_token, vocab_size]\n",
    "            elif dis_metric == 'euc':\n",
    "                sim = - torch.cdist(mol_tokens.flatten(0, 1), embed_weight, p=2)\n",
    "                assert sim.shape == (B * num_q, embed_weight.shape[0])\n",
    "            else:\n",
    "                raise NotImplementedError()\n",
    "            _, topk_ids = torch.topk(sim, k=topk, dim=-1) # shape = [mol_num * num_query_token, k]\n",
    "            knn_decode_strings = self.opt_tokenizer.batch_decode(topk_ids.flatten())\n",
    "            knn_decode_strings = np.asarray(knn_decode_strings).reshape(B, num_q, topk).tolist() # shape = [mol_num, num_query_token, topk]\n",
    "            knn_decode_strings = [[' '.join(ii) for ii in i] for i in knn_decode_strings] # shape = [mol_num, num_query_token]\n",
    "            if False:\n",
    "                ### print for presentation\n",
    "                assert len(knn_decode_strings) == len(texts)\n",
    "                for predict, text in zip(knn_decode_strings, texts):\n",
    "                    print('----------------------------')\n",
    "                    print(predict)\n",
    "                    print(text)\n",
    "            return knn_decode_strings\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/home/ac15y5ara4/cz/MolTC-main/model/blip2.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load('/work/home/ac15y5ara4/cz/MolTC-main/gin_pretrained/graphcl_80.pth', map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert load scibert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_49365/3830296720.py:524: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
      "  device = XXXX-2(self.parameters()).device\n",
      "/tmp/ipykernel_49365/3830296720.py:693: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
      "  device = XXXX-2(self.parameters()).device\n",
      "/tmp/ipykernel_49365/3830296720.py:524: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
      "  device = XXXX-2(self.parameters()).device\n",
      "/tmp/ipykernel_49365/3830296720.py:693: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
      "  device = XXXX-2(self.parameters()).device\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The state dictionary of the model you are trying to load is corrupted. Are you sure it was properly saved?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m blip2opt \u001b[38;5;241m=\u001b[39m \u001b[43mBlip2OPT\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgin_num_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgin_hidden_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtune_gnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_query_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_attention_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_tune\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpeft_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 65\u001b[0m, in \u001b[0;36mBlip2OPT.__init__\u001b[0;34m(self, bert_name, gin_num_layers, gin_hidden_dim, gin_drop_ratio, tune_gnn, num_query_token, cross_attention_freq, llm_tune, peft_dir, opt_model, prompt, args)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_model \u001b[38;5;241m=\u001b[39m OPTForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(opt_model)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# self.opt_model = OPTForCausalLM.from_pretrained(opt_model, torch_dtype=torch.float16)\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_model \u001b[38;5;241m=\u001b[39m \u001b[43mOPTForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_model\u001b[38;5;241m.\u001b[39mresize_token_embeddings(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_tokenizer)) \u001b[38;5;66;03m## this will cause bug when full fine-tuning the opt model\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_tune \u001b[38;5;241m=\u001b[39m llm_tune\n",
      "File \u001b[0;32m/opt/conda/envs/deepseek/lib/python3.10/site-packages/transformers/modeling_utils.py:3852\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3843\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3844\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3845\u001b[0m     (\n\u001b[1;32m   3846\u001b[0m         model,\n\u001b[1;32m   3847\u001b[0m         missing_keys,\n\u001b[1;32m   3848\u001b[0m         unexpected_keys,\n\u001b[1;32m   3849\u001b[0m         mismatched_keys,\n\u001b[1;32m   3850\u001b[0m         offload_index,\n\u001b[1;32m   3851\u001b[0m         error_msgs,\n\u001b[0;32m-> 3852\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[1;32m   3853\u001b[0m         model,\n\u001b[1;32m   3854\u001b[0m         state_dict,\n\u001b[1;32m   3855\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3856\u001b[0m         resolved_archive_file,\n\u001b[1;32m   3857\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3858\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m   3859\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[1;32m   3860\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[1;32m   3861\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m   3862\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   3863\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   3864\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m   3865\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   3866\u001b[0m         is_quantized\u001b[39m=\u001b[39;49m(\u001b[39mgetattr\u001b[39;49m(model, \u001b[39m\"\u001b[39;49m\u001b[39mquantization_method\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m) \u001b[39m==\u001b[39;49m QuantizationMethod\u001b[39m.\u001b[39;49mBITS_AND_BYTES),\n\u001b[1;32m   3867\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   3868\u001b[0m     )\n\u001b[1;32m   3870\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_4bit \u001b[39m=\u001b[39m load_in_4bit\n\u001b[1;32m   3871\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_8bit \u001b[39m=\u001b[39m load_in_8bit\n",
      "File \u001b[0;32m/opt/conda/envs/deepseek/lib/python3.10/site-packages/transformers/modeling_utils.py:4148\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   4146\u001b[0m base_model_expected_keys \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(model_to_load\u001b[39m.\u001b[39mstate_dict()\u001b[39m.\u001b[39mkeys())\n\u001b[1;32m   4147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(key \u001b[39min\u001b[39;00m expected_keys_not_prefixed \u001b[39mand\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m base_model_expected_keys \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m loaded_keys):\n\u001b[0;32m-> 4148\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   4149\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe state dictionary of the model you are trying to load is corrupted. Are you sure it was \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4150\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mproperly saved?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4151\u001b[0m     )\n\u001b[1;32m   4152\u001b[0m \u001b[39mif\u001b[39;00m device_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   4153\u001b[0m     device_map \u001b[39m=\u001b[39m {k\u001b[39m.\u001b[39mreplace(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mbase_model_prefix\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m): v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m device_map\u001b[39m.\u001b[39mitems()}\n",
      "\u001b[0;31mValueError\u001b[0m: The state dictionary of the model you are trying to load is corrupted. Are you sure it was properly saved?"
     ]
    }
   ],
   "source": [
    "\n",
    "blip2opt = Blip2OPT(args.bert_name, args.gin_num_layers, args.gin_hidden_dim, args.drop_ratio, \n",
    "args.tune_gnn, args.num_query_token, args.cross_attention_freq, args.llm_tune, args.peft_dir, args.opt_model, args.prompt, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/cz/MolTC-main/all_checkpoints/oridaima/epoch=09.ckpt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m checkpoint_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/cz/MolTC-main/all_checkpoints/oridaima/epoch=09.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/moltc/lib/python3.10/site-packages/torch/serialization.py:1065\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1063\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1065\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1067\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/envs/moltc/lib/python3.10/site-packages/torch/serialization.py:468\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 468\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/envs/moltc/lib/python3.10/site-packages/torch/serialization.py:449\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/cz/MolTC-main/all_checkpoints/oridaima/epoch=09.ckpt'"
     ]
    }
   ],
   "source": [
    "# checkpoint_path = \"/home/cz/MolTC-main/all_checkpoints/oridaima/epoch=09.ckpt\"\n",
    "# checkpoint = torch.load(checkpoint_path)\n",
    "# state_dict = checkpoint['state_dict']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'state_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m blip2opt\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mstate_dict\u001b[49m,strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'state_dict' is not defined"
     ]
    }
   ],
   "source": [
    "blip2opt.load_state_dict(state_dict,strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Blip2OPT(\n",
       "  (graph_encoder): GNN(\n",
       "    (x_embedding1): Embedding(120, 300)\n",
       "    (x_embedding2): Embedding(3, 300)\n",
       "    (gnns): ModuleList(\n",
       "      (0-4): 5 x GINConv()\n",
       "    )\n",
       "    (batch_norms): ModuleList(\n",
       "      (0-4): 5 x BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_graph): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "  (Qformer): BertLMHeadModel(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): None\n",
       "        (position_embeddings): None\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=300, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=300, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): None\n",
       "            (output): None\n",
       "            (intermediate_query): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output_query): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): None\n",
       "            (output): None\n",
       "            (intermediate_query): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output_query): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=300, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=300, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): None\n",
       "            (output): None\n",
       "            (intermediate_query): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output_query): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): None\n",
       "            (output): None\n",
       "            (intermediate_query): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output_query): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=300, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=300, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): None\n",
       "            (output): None\n",
       "            (intermediate_query): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output_query): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): None\n",
       "            (output): None\n",
       "            (intermediate_query): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output_query): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=300, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=300, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): None\n",
       "            (output): None\n",
       "            (intermediate_query): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output_query): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): None\n",
       "            (output): None\n",
       "            (intermediate_query): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output_query): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=300, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=300, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): None\n",
       "            (output): None\n",
       "            (intermediate_query): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output_query): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): None\n",
       "            (output): None\n",
       "            (intermediate_query): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output_query): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=300, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=300, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): None\n",
       "            (output): None\n",
       "            (intermediate_query): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output_query): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): None\n",
       "            (output): None\n",
       "            (intermediate_query): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output_query): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): None\n",
       "  )\n",
       "  (opt_model): PeftModelForCausalLM(\n",
       "    (base_model): LoraModel(\n",
       "      (model): OPTForCausalLM(\n",
       "        (model): OPTModel(\n",
       "          (decoder): OPTDecoder(\n",
       "            (embed_tokens): Embedding(50001, 2048, padding_idx=1)\n",
       "            (embed_positions): OPTLearnedPositionalEmbedding(2050, 2048)\n",
       "            (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "            (layers): ModuleList(\n",
       "              (0-23): 24 x OPTDecoderLayer(\n",
       "                (self_attn): OPTAttention(\n",
       "                  (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "                  (v_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (q_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "                )\n",
       "                (activation_fn): GELUActivation()\n",
       "                (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "                (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "                (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "                (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (lm_head): Linear(in_features=2048, out_features=50001, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (opt_proj): Linear(in_features=768, out_features=2048, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blip2opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_provider.stage2_dm import Stage2DM,Stage2DM_double,Stage2DM_double_value,Stage2DM_double_DDIvalue,Stage2DM_double_fgtvalue,Stage2DM_universal\n",
    "tokenizer = blip2opt.opt_tokenizer\n",
    "dm = Stage2DM_double(args.mode, args.num_workers, args.batch_size, '/home/cz/MolTC-main/data/ddi_data/Zhangddi_data/train/', args.text_max_len, \n",
    "tokenizer, args.zijiegou,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "train_loader = dm.val_dataloader()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(DataBatch(x=[613, 2], edge_index=[2, 1312], edge_attr=[1312, 2], rdkit_indices=[613], batch=[613], ptr=[21]),\n",
       " DataBatch(x=[507, 2], edge_index=[2, 1092], edge_attr=[1092, 2], rdkit_indices=[507], batch=[507], ptr=[21]),\n",
       " {'input_ids': tensor([[   2,  243,   21,  ...,    1,    1,    1],\n",
       "         [   2,  243,   21,  ...,    1,    1,    1],\n",
       "         [   2,  243,   21,  ...,    1,    1,    1],\n",
       "         ...,\n",
       "         [   2,  243,   21,  ...,    1,    1,    1],\n",
       "         [   2,  243,   21,  ...,   36, 6402,   48],\n",
       "         [   2,  243,   21,  ...,    1,    1,    1]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]), 'is_mol_token': tensor([[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]])},\n",
       " {'input_ids': tensor([[   25,    58, 21934,    39,   312,   243,    25,    58, 21934,    40,\n",
       "            917,   530,  5818,  2269,    35, 12743,  2155,    36],\n",
       "         [   25,    58, 21934,    39,   312,   243,    25,    58, 21934,    40,\n",
       "            917,   530,  5818,  2269,    35, 12743,  2155,    36],\n",
       "         [   25,    58, 21934,    39,   312,   243,    25,    58, 21934,    40,\n",
       "            917,   530,  5818,  2269,    35, 12743,  2155,    36],\n",
       "         [   25,    58, 21934,    39,   312,   243,    25,    58, 21934,    40,\n",
       "           5818,  2269,    35, 12743,  2155,    36,     1,     1],\n",
       "         [   25,    58, 21934,    39,   312,   243,    25,    58, 21934,    40,\n",
       "            917,   530,  5818,  2269,    35, 12743,  2155,    36],\n",
       "         [   25,    58, 21934,    39,   312,   243,    25,    58, 21934,    40,\n",
       "            917,   530,  5818,  2269,    35, 12743,  2155,    36],\n",
       "         [   25,    58, 21934,    39,   312,   243,    25,    58, 21934,    40,\n",
       "            917,   530,  5818,  2269,    35, 12743,  2155,    36],\n",
       "         [   25,    58, 21934,    39,   312,   243,    25,    58, 21934,    40,\n",
       "            917,   530,  5818,  2269,    35, 12743,  2155,    36],\n",
       "         [   25,    58, 21934,    39,   312,   243,    25,    58, 21934,    40,\n",
       "           5818,  2269,    35, 12743,  2155,    36,     1,     1],\n",
       "         [   25,    58, 21934,    39,   312,   243,    25,    58, 21934,    40,\n",
       "            917,   530,  5818,  2269,    35, 12743,  2155,    36],\n",
       "         [   25,    58, 21934,    39,   312,   243,    25,    58, 21934,    40,\n",
       "            917,   530,  5818,  2269,    35, 12743,  2155,    36],\n",
       "         [   25,    58, 21934,    39,   312,   243,    25,    58, 21934,    40,\n",
       "            917,   530,  5818,  2269,    35, 12743,  2155,    36],\n",
       "         [   25,    58, 21934,    39,   312,   243,    25,    58, 21934,    40,\n",
       "           5818,  2269,    35, 12743,  2155,    36,     1,     1],\n",
       "         [   25,    58, 21934,    39,   312,   243,    25,    58, 21934,    40,\n",
       "            917,   530,  5818,  2269,    35, 12743,  2155,    36],\n",
       "         [   25,    58, 21934,    39,   312,   243,    25,    58, 21934,    40,\n",
       "            917,   530,  5818,  2269,    35, 12743,  2155,    36],\n",
       "         [   25,    58, 21934,    39,   312,   243,    25,    58, 21934,    40,\n",
       "            917,   530,  5818,  2269,    35, 12743,  2155,    36],\n",
       "         [   25,    58, 21934,    39,   312,   243,    25,    58, 21934,    40,\n",
       "            917,   530,  5818,  2269,    35, 12743,  2155,    36],\n",
       "         [   25,    58, 21934,    39,   312,   243,    25,    58, 21934,    40,\n",
       "            917,   530,  5818,  2269,    35, 12743,  2155,    36],\n",
       "         [   25,    58, 21934,    39,   312,   243,    25,    58, 21934,    40,\n",
       "            917,   530,  5818,  2269,    35, 12743,  2155,    36],\n",
       "         [   25,    58, 21934,    39,   312,   243,    25,    58, 21934,    40,\n",
       "           5818,  2269,    35, 12743,  2155,    36,     1,     1]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = next(iter(train_loader))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs1,graphs2, prompt_tokens, texts = a\n",
    "#  graphs1 = graphs1.to(torch.bfloat16)\n",
    "# graphs2 = graphs2.to(torch.bfloat16)\n",
    "# prompt_tokens = prompt_tokens.to(torch.bfloat16)\n",
    "samples = {'graphs1': graphs1, 'graphs2': graphs2,'prompt_tokens': prompt_tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 228])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[\"prompt_tokens\"][\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_tokens.is_mol_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始生成\n"
     ]
    }
   ],
   "source": [
    "predictions = blip2opt.generate(\n",
    "            samples, \n",
    "            do_sample=True,\n",
    "            num_beams=args.num_beams-3,\n",
    "            max_length=11,\n",
    "            min_length=args.min_len,\n",
    "            num_captions=1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.hidden_states[-1][5][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "print(len(predictions.hidden_states))\n",
    "print(len(predictions.hidden_states[-1]))\n",
    "# print(len(predictions.hidden_states[-1][0]))\n",
    "#maxlen-1 * hidden_layers *(batchsize*numbeams*captions)*1*2048\n",
    "a=predictions.hidden_states[-4][-1][::2].squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模拟的 caption_outputs 数据\n",
    "caption_outputs = [\n",
    "    (\n",
    "        [\"prediction_1a\", \"prediction_1b\"],   # list_predictions\n",
    "        [\"target_1a\", \"target_1b\"],           # list_targets\n",
    "        torch.tensor([1, 0])              # list_mlppredicts\n",
    "    ),\n",
    "    (\n",
    "        [\"prediction_2a\", \"prediction_2b\"],\n",
    "        [\"target_2a\", \"target_2b\"],\n",
    "        torch.tensor([0, 1])\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: ['prediction_1a', 'prediction_1b', 'prediction_2a', 'prediction_2b']\n",
      "Targets: ['target_1a', 'target_1b', 'target_2a', 'target_2b']\n",
      "MLP Predicts: [1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# 解包数据\n",
    "list_predictions, list_targets, list_mlppredicts = zip(*caption_outputs)\n",
    "\n",
    "# 展开 predictions 和 targets 列表\n",
    "predictions = [i for ii in list_predictions for i in ii]\n",
    "targets = [i for ii in list_targets for i in ii]\n",
    "mlppredicts = [i.item() for ii in list_mlppredicts for i in ii]  # 转为标量列表\n",
    "\n",
    "# 输出结果\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"Targets:\", targets)\n",
    "print(\"MLP Predicts:\", mlppredicts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 2048])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,   304,   381,  2351,   343,   281,   243,    39,    42,    35,\n",
       "          4293],\n",
       "        [    0,   473, 35427,  4545,   425,    35,    62,    40,    68,    39,\n",
       "            62],\n",
       "        [    0,    30,    39,    31,   381,  1093,  6655,   343,  3129,   345,\n",
       "           281],\n",
       "        [    0,   111,   606,    85,  5145,    85,  6231,   243,    51,   243,\n",
       "            29],\n",
       "        [    0,    30,   592,    31,    68,    34,    68,    35, 21882,  3082,\n",
       "          6246],\n",
       "        [    0,   325,   851,   243,    51,   243,    30,    24,    63,    24,\n",
       "           243],\n",
       "        [    0,    30,    63,  1361,   568,   774,  5570,    31,   221,     1,\n",
       "             1],\n",
       "        [    0,   105,   243,    35,  5410,   343,   286,  2456,  2157,   299,\n",
       "          1199],\n",
       "        [    0,    36,   732,  5262,   496,   391, 20673,   452,   243,    39,\n",
       "            40],\n",
       "        [    0,   243,    44,    38,  2303,   343,  1142,    36,  5987,  1652,\n",
       "         14837],\n",
       "        [    0,   325,   851,   243,    36,   243,    26,    39,    26,   221,\n",
       "             1],\n",
       "        [    0,    48, 22381,   299,   773,   343,   530,  2315,    48,   221,\n",
       "             1],\n",
       "        [    0,    94,   573,    85, 32492,    48,   479,   286,  1601,    34,\n",
       "           281],\n",
       "        [    0,   243,    39,    38,    36,  8996,   343,   286,   781,   299,\n",
       "           286],\n",
       "        [    0,    36,   862,    85,   593,    49,   369,  8197,   495,  2706,\n",
       "           343],\n",
       "        [    0,   243,  2298,     2,   927,    48,   243,    81,    57, 11426,\n",
       "         22588],\n",
       "        [    0,    30,    55,    31,   592,  2099,   299,   286,  1045,  2792,\n",
       "           299],\n",
       "        [    0,    48,   381,  1093,   417,  4491,   299,  4892,   345,   286,\n",
       "          3170],\n",
       "        [    0,  5410,    29,   105,   286,  1008,   299,   281,   243,    24,\n",
       "          1072],\n",
       "        [    0,    51,    38,    36,    39,    39,    42,    39,    34, 50000,\n",
       "            30]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 1, 2048])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.hidden_states[-4][-1][:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 2048])\n"
     ]
    }
   ],
   "source": [
    "# 获取生成的序列和隐藏状态\n",
    "sequences = predictions.sequences  # (batch_size*num_return_sequences, sequence_length)\n",
    "hidden_states = predictions.hidden_states  # Tuple of shape (sequence_length, num_layers, (batch_size*num_beams*num_return_sequences, generated_length, hidden_size))\n",
    "\n",
    "# 获取最后一层隐藏状态\n",
    "# hidden_states[-1] 是最后生成的 token 的隐藏状态\n",
    "# hidden_states[-1][-1] 是最后一层的隐藏状态\n",
    "last_hidden_states = hidden_states[-1][-1]  # Shape: (batch_size*num_return_sequences, sequence_length, hidden_size)\n",
    "\n",
    "# 如果需要获取每个 token 的最后一层隐藏状态\n",
    "final_hidden_states_per_token = last_hidden_states[:, :sequences.shape[1], :]  # 对应每个生成的 token 的最后一层隐藏状态\n",
    "\n",
    "print(final_hidden_states_per_token.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
